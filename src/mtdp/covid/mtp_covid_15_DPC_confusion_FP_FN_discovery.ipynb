{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from platform import python_version\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Comparing Default Cutoff x BCA: TP, FP, TN, FN\n",
    "\n",
    "  - Given the default enriched table (tab1)\n",
    "  - Given the BCA enriched Table, removing the default pathways = extra pathways (tab2)\n",
    "\n",
    "### TP and FP\n",
    "   - tab1\n",
    "     - Gemini Yes --> TP\n",
    "     - Gemini False --> FP\n",
    "    \n",
    "### TN and FN\n",
    "   - tab2\n",
    "     - Gemini Yes --> FN\n",
    "     - Gemini False --> TN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Comparing G0 x G1 pathways: TP, FP, TN, FN\n",
    "\n",
    "  - Given the G0 table (tab1)\n",
    "  - Given the G1 table(tab2)\n",
    "\n",
    "### TP and FP\n",
    "   - tab1\n",
    "     - Gemini Yes --> TP\n",
    "     - Gemini False --> FP\n",
    "    \n",
    "### TN and FN\n",
    "   - tab2\n",
    "     - Gemini Yes --> FN\n",
    "     - Gemini False --> TN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.width', 100)\n",
    "pd.set_option('max_colwidth', 80)\n",
    "pd.set_option(\"display.precision\", 3)\n",
    "\n",
    "import yaml\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_context(\"notebook\", font_scale=1.4)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "%matplotlib inline\n",
    "\n",
    "sys.path.insert(1, '../src/')\n",
    "\n",
    "from Basic import *\n",
    "from biopax_lib import *\n",
    "from gemini_lib import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>:root { --jp-notebook-max-width: 100% !important; }</style>\"))\n",
    "\n",
    "with open('params.yml', 'r') as file:\n",
    "    dic_yml=yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root0=dic_yml['root0']\n",
    "root_data_aux=dic_yml['root_data_aux']\n",
    "email=dic_yml['email']\n",
    "\n",
    "project=dic_yml['project']\n",
    "s_project=dic_yml['s_project']\n",
    "\n",
    "gene_protein=dic_yml['gene_protein']\n",
    "s_omics=dic_yml['s_omics']\n",
    "\n",
    "has_age=dic_yml['has_age']\n",
    "has_gender=dic_yml['has_gender']\n",
    "\n",
    "want_normalized=dic_yml['want_normalized']\n",
    "\n",
    "abs_lfc_cutoff_inf=dic_yml['abs_lfc_cutoff_inf']\n",
    "s_pathw_enrichm_method=dic_yml['s_pathw_enrichm_method']\n",
    "num_min_degs_for_ptw_enr=dic_yml['num_min_degs_for_ptw_enr']\n",
    "\n",
    "tolerance_pathway_index=dic_yml['tolerance_pathway_index']\n",
    "type_sat_ptw_index=dic_yml['type_sat_ptw_index']\n",
    "saturation_lfc_index=dic_yml['saturation_lfc_index']\n",
    "chosen_model_sampling=dic_yml['chosen_model_sampling']\n",
    "\n",
    "case_list=dic_yml['case_list']\n",
    "case_sel_list=dic_yml['case_sel_list']\n",
    "\n",
    "pval_pathway_cutoff=dic_yml['pval_pathway_cutoff']\n",
    "fdr_pathway_cutoff=dic_yml['fdr_pathway_cutoff']\n",
    "num_of_genes_cutoff=dic_yml['num_of_genes_cutoff']\n",
    "\n",
    "run_list=dic_yml['run_list']\n",
    "chosen_model_list=dic_yml['chosen_model_list']\n",
    "i_dfp_list=dic_yml['i_dfp_list']\n",
    "\n",
    "exp_normalization='quantile_norm' if want_normalized else None\n",
    "normalization='not_normalized' if exp_normalization is None else exp_normalization\n",
    "\n",
    "cfg=Config(project, s_project, case_list, root0)\n",
    "\n",
    "case=case_list[0]\n",
    "\n",
    "n_genes_annot_ptw, n_degs, n_degs_in_ptw, n_degs_not_in_ptw, degs_in_all_ratio=-1,-1,-1,-1,-1\n",
    "abs_lfc_cutoff, fdr_lfc_cutoff, n_degs, n_degs_up, n_degs_dw=cfg.get_best_lfc_cutoff(case, 'not_normalized')\n",
    "\n",
    "print(f\"G/P LFC cutoffs: lfc={abs_lfc_cutoff:.3f}; fdr={fdr_lfc_cutoff:.3f}\")\n",
    "print(f\"Pathway cutoffs: pval={pval_pathway_cutoff:.3f}; fdr={fdr_pathway_cutoff:.3f}; num of genes={num_of_genes_cutoff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpx=Biopax(gene_protein, s_omics, project, s_project, root0,\n",
    "           case_list, has_age, has_gender, clone_objects=False,\n",
    "           exp_normalization=exp_normalization, geneset_num=0, \n",
    "           num_min_degs_for_ptw_enr=num_min_degs_for_ptw_enr, \n",
    "           tolerance_pathway_index=tolerance_pathway_index, \n",
    "           s_pathw_enrichm_method=s_pathw_enrichm_method,\n",
    "           abs_lfc_cutoff_inf=abs_lfc_cutoff_inf, \n",
    "           type_sat_ptw_index=type_sat_ptw_index, saturation_lfc_index=saturation_lfc_index)\n",
    "\n",
    "case=case_list[0]\n",
    "\n",
    "bpx.cfg.set_default_best_lfc_cutoff(normalization, abs_lfc_cutoff=1, fdr_lfc_cutoff=0.05)\n",
    "ret, degs, degs_ensembl, dfdegs=bpx.open_case(case, verbose=False)\n",
    "print(\"\\nEcho Parameters:\")\n",
    "bpx.echo_parameters()\n",
    "\n",
    "geneset_num=bpx.geneset_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble: is_seldata=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "is_seldata=False\n",
    "\n",
    "with_gender=bpx.has_gender\n",
    "with_gender_list = [False, True] if with_gender else [False]\n",
    "\n",
    "print(f\"with_gender = {with_gender} because has_gender = {bpx.has_gender}\")\n",
    "###################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiating Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY=dic_yml['API_KEY']\n",
    "\n",
    "disease=dic_yml['disease']\n",
    "context_disease=dic_yml['context_disease']\n",
    "n_sentences=dic_yml['n_sentences']\n",
    "chosen_model_sampling=dic_yml['chosen_model_sampling']\n",
    "\n",
    "gem=Gemini(bpx=bpx, is_seldata=is_seldata, disease=disease, context_disease=context_disease, \n",
    "           API_KEY=API_KEY, n_sentences=n_sentences, root0=root0, \n",
    "           chosen_model_list=chosen_model_list, i_dfp_list=i_dfp_list, chosen_model_sampling=chosen_model_sampling)\n",
    "print(\"\\n\")\n",
    "print(gem.disease, gem.is_seldata, gem.i_dfp_list, gem.chosen_model_list)\n",
    "print(\"Context:\", context_disease)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_seldata, case_list, run_list, chosen_model_list, i_dfp_list, chosen_model_sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gem.set_case(bpx.case, bpx.df_enr, bpx.df_enr0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gem.is_seldata, gem.bpx.case_list, gem.chosen_model_list, gem.i_dfp_list, gem.chosen_model_sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose=True\n",
    "\n",
    "run='run01'\n",
    "case=case_list[0]\n",
    "print(\">>>\", case, '\\n')\n",
    "\n",
    "dfpiva = gem.open_gemini_dfpiva_all_models_one_run(run=run, chosen_model_list=chosen_model_list, verbose=verbose)\n",
    "dfpiv = dfpiva[(dfpiva.case == case) & (dfpiva.i_dfp == 0)]\n",
    "\n",
    "dfpiv.consensus.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Statistics Default x BCA: confusion table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose=True\n",
    "force=False\n",
    "\n",
    "run='run01'\n",
    "case = case_list[0]\n",
    "# group_discovery_fp_fn_enriched_bca\n",
    "df, conf_list = gem.confusion_table_fp_fn_enriched_bca(run=run, case=case, chosen_model_list=chosen_model_list, force=force, verbose=verbose)\n",
    "\n",
    "print(case, conf_list, '\\n')\n",
    "print(len(df))\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('group').count().reset_index().iloc[:,:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "verbose=False\n",
    "force=False\n",
    "\n",
    "run='run01'\n",
    "\n",
    "for case in case_list:\n",
    "    df, conf_list = gem.confusion_table_fp_fn_enriched_bca(run=run, case=case, chosen_model_list=chosen_model_list, force=force, verbose=verbose)\n",
    "\n",
    "print(case, conf_list, '\\n')\n",
    "print(len(df))\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('group').count().reset_index().iloc[:,:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "force=False\n",
    "verbose=True\n",
    "prompt=False\n",
    "param_perc=0.9\n",
    "\n",
    "\n",
    "dfa = gem.calc_confusion_stats_enriched_bca_per_run_case(run=run, case_list=case_list, chosen_model_list=chosen_model_list, \n",
    "                                                         param_perc=param_perc, prompt=prompt, force=force, verbose=verbose)\n",
    "dfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Statistics G0 x G1: confusion table\n",
    "  - Positive control: i_dfp==0\n",
    "  - fuzzy negative1: i_dfp==1\n",
    "  - fuzzy negative2: i_dfp==2\n",
    "  - negative control: i_dfp==3\n",
    "  - calc:\n",
    "    - TP, FP: positive control\n",
    "    - TN1, FN1: fuzzy negative1\n",
    "    - TN2, FN2: fuzzy negative2\n",
    "    - TN, FN: negative control\n",
    "    - calc: Sensitivity, Specificity, Accuracy, and F1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text, dfcons = gem.open_gemini_summary_consensus_statitics_idfp(chosen_model_list=chosen_model_list, verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcons.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose=True\n",
    "force=False\n",
    "\n",
    "dfconf1, dfconf2, dfconf3 = \\\n",
    "    gem.calc_gemini_4groups_confusion_table(run_list=run_list, case_list=case_list,\n",
    "                                            chosen_model_list=chosen_model_list,\n",
    "                                            force=force, verbose=verbose)\n",
    "\n",
    "if dfconf1 is None:\n",
    "    print(\"Could not calcualte\")\n",
    "    df3 = pd.DataFrame()\n",
    "else:\n",
    "    cols = ['case', 'which', 'n', 'npos', 'nneg', 'TP', 'FP', 'TN', 'FN', 'sens', 'spec', 'accu', 'prec', 'f1_score']\n",
    "    \n",
    "    run = run_list[0]\n",
    "    \n",
    "    df1 = dfconf1[dfconf1.run == run][cols]\n",
    "    df2 = dfconf2[dfconf2.run == run][cols]\n",
    "    df3 = dfconf3[dfconf3.run == run][cols]\n",
    "\n",
    "    print(\"Ok\")\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:,.3f}'.format\n",
    "\n",
    "print(\">>>\", run, '\\n')\n",
    "mu_f1 = df1.f1_score.mean()\n",
    "std_f1 = df1.f1_score.std()\n",
    "print(f\"G0 x G1: mu_f1 {100*mu_f1:.1f}% ({100*std_f1:.1f}%)\\n\")\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\">>>\", run, '\\n')\n",
    "mu_f1 = df2.f1_score.mean()\n",
    "std_f1 = df2.f1_score.std()\n",
    "print(f\"G0 x G2: mu_f1 {100*mu_f1:.1f}% ({100*std_f1:.1f}%)\\n\")\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\">>>\", run, '\\n')\n",
    "mu_f3 = df3.f1_score.mean()\n",
    "std_f3 = df3.f1_score.std()\n",
    "print(f\"G0 x G3: mu_f3 {100*mu_f3:.1f}% ({100*std_f3:.1f}%)\\n\")\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = [('sens', 'Sensitivity'), ('spec', 'Specificity'), ('accu', 'Accuracy'),\n",
    "\t   ('prec', 'Precision'), ('f1_score', 'F1-score')]\n",
    "\n",
    "for i in range(3):\n",
    "    if i==0:\n",
    "        dfa = df1\n",
    "        dfb = df2\n",
    "        compare = 'G0xG1 x G0xG2'\n",
    "    elif i==1:\n",
    "        dfa = df1\n",
    "        dfb = df3\n",
    "        compare = 'G0xG1 x G0xG3'\n",
    "    else:\n",
    "        dfa = df2\n",
    "        dfb = df3\n",
    "        compare = 'G0xG2 x G0xG3'\n",
    "\n",
    "    for col, test in mat:\n",
    "        # print(dfa[col])\n",
    "        # print(dfb[col])\n",
    "        \n",
    "        mua  = dfa[col].mean()\n",
    "        stda = dfa[col].std()\n",
    "\n",
    "        mub  = dfb[col].mean()\n",
    "        stdb = dfb[col].std()\n",
    "        \n",
    "        s_stat, stat, pval = calc_ttest(dfa[col], dfb[col])\n",
    "        print(f\"{compare}: {100*mua:.1f}% ({100*stda:.1f}%) x {100*mub:.1f}% ({100*stdb:.1f}%) -> {test:12} pval {pval:.2e}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_stat, stat, pval = calc_ttest(df1[col], df3[col])\n",
    "s_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_stat, stat, pval = calc_ttest(df2[col], df3[col])\n",
    "s_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['run', 'which', 'case', 'n', 'npos', 'nneg', 'TP', 'FP', 'TN', 'FN', \n",
    "        'sens', 'spec', 'accu', 'prec', 'f1_score']\n",
    "\n",
    "cols = ['run', 'which', 'case', 'n', 'npos', 'nneg', 'TP', 'FP', 'TN', 'FN', \n",
    "        'sens', 'spec', 'accu', 'prec', 'f1_score']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfconf1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run=run_list[1]\n",
    "\n",
    "df11 = dfconf1[dfconf1.run == run][cols]\n",
    "df12 = dfconf2[dfconf2.run == run][cols]\n",
    "df13 = dfconf3[dfconf3.run == run][cols]\n",
    "\n",
    "print(\">>>\", run, '\\n')\n",
    "df13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run='run01'\n",
    "verbose=True\n",
    "force=True\n",
    "\n",
    "df, df1, df2, df3 = gem.calc_gemini_4groups_confusion_stats(run=run, run_list=run_list,\n",
    "                                                            case_list=case_list, chosen_model_list=chosen_model_list,\n",
    "                                                            alpha=0.05, force=force, verbose=verbose)\n",
    "\n",
    "print(len(df))\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = 'sens'\n",
    "compare = 'G1xG0'\n",
    "df[(df.run == run) & (df.test == col)   ] #  & (df['compare'] == compare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_bar_errors=True\n",
    "\n",
    "fig = gem.barplot_comparing_confusion_groups(run=run, run_list=run_list, case_list=case_list,\n",
    "                                       chosen_model_list=chosen_model_list,\n",
    "                                       width=1100, height=500, \n",
    "                                       fontsize=14, fontcolor='black',\n",
    "                                       margin=dict( l=20, r=20, b=100, t=120, pad=4),\n",
    "                                       plot_bgcolor=\"whitesmoke\",\n",
    "                                       xaxis_title=\"parameters\", yaxis_title='percentage (%)',\n",
    "                                       minus_test=-.2, minus_group=-0.1, \n",
    "                                       annot_fontfamily=\"Arial, monospace\", annot_fontsize=12, \n",
    "                                       annot_fontcolor='black', show_bar_errors=show_bar_errors,\n",
    "                                       savePlot=True, force=force, verbose=verbose)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df))\n",
    "\n",
    "# cols = ['run', 'test', 'test_desc', 'compare', 'n', 'mu', 'std', 'error', 'SEM', 'cinf', 'csup', 'pval', 'asteristics', 'stat', 's_stat']\n",
    "cols = ['test_desc', 'compare', 'n', 'mu', 'std', 'error', 'SEM', 'cinf', 'csup', 'pval', 'pval_bonf', 'asteristics', 'stat', 's_stat']\n",
    "df[df.test_desc == 'Sensitivity'][cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing runs: stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose=True\n",
    "force=False\n",
    "\n",
    "# calc_stats_gemini_4groups_confusion_table\n",
    "dfstat = gem.calc_stats_gemini_4groups_confusion_compare_runs(run_list=run_list, case_list=case_list,\n",
    "                                                              chosen_model_list=chosen_model_list,\n",
    "                                                              force=force, verbose=verbose)\n",
    "\n",
    "# cols = ['run0', 'run1', 'which', 'test', 'test_ext', 'mu_param0', 'std_param0', 'mu_param1', 'std_param1', 'stat_test', 'pvalue', 'stat', 'text_stat', 'text_ext']\n",
    "dfstat.test_ext.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['which', 'test_ext', 'mu_param0', 'std_param0', 'mu_param1', 'std_param1', 'stat_test', 'pvalue', 'stat', 'text_ext']\n",
    "tests = ['Sensitivity', 'Specificity', 'Accuracy', 'Precision', 'F1-score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfstat[dfstat.test_ext == tests[0]][cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfstat[dfstat.test_ext == tests[1]][cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfstat[dfstat.test_ext == tests[2]][cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfstat[dfstat.test_ext == tests[3]][cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfstat[dfstat.test_ext == tests[4]][cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run='run01'\n",
    "case=case_list[0]\n",
    "df, conf_list = gem.confusion_table_fp_fn_enriched_bca(run=run, case=case, chosen_model_list=chosen_model_list, force=force, verbose=verbose)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp, fp, tn, fn = conf_list\n",
    "tp, fp, tn, fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfpiva = gem.open_gemini_dfpiva_all_models_one_run(run=run, chosen_model_list=chosen_model_list, \n",
    "                                                    verbose=verbose)\n",
    "dfpiva.i_dfp.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
