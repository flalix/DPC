{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from platform import python_version\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Comparing Default Cutoff x BCA: TP, FP, TN, FN\n",
    "\n",
    "  - Given the default enriched table (tab1)\n",
    "  - Given the BCA enriched Table, removing the default pathways = extra pathways (tab2)\n",
    "\n",
    "### TP and FP\n",
    "   - tab1\n",
    "     - Gemini Yes --> TP\n",
    "     - Gemini False --> FP\n",
    "    \n",
    "### TN and FN\n",
    "   - tab2\n",
    "     - Gemini Yes --> FN\n",
    "     - Gemini False --> TN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Comparing G0 x G1 pathways: TP, FP, TN, FN\n",
    "\n",
    "  - Given the G0 table (tab1)\n",
    "  - Given the G1 table(tab2)\n",
    "\n",
    "### TP and FP\n",
    "   - tab1\n",
    "     - Gemini Yes --> TP\n",
    "     - Gemini False --> FP\n",
    "    \n",
    "### TN and FN\n",
    "   - tab2\n",
    "     - Gemini Yes --> FN\n",
    "     - Gemini False --> TN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.width', 100)\n",
    "pd.set_option('max_colwidth', 80)\n",
    "import yaml\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_context(\"notebook\", font_scale=1.4)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "%matplotlib inline\n",
    "\n",
    "sys.path.insert(1, '../src/')\n",
    "\n",
    "from Basic import *\n",
    "from entrez_conversion import *\n",
    "from pubmed_lib import *\n",
    "from biopax_lib import *\n",
    "from gemini_lib import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "# display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "display(HTML(\"<style>:root { --jp-notebook-max-width: 100% !important; }</style>\"))\n",
    "\n",
    "email = \"flalix@gmail.com\"\n",
    "\n",
    "# !pip3 install pyyaml\n",
    "with open('params.yml', 'r') as file:\n",
    "    dic_yml = yaml.safe_load(file)\n",
    "\n",
    "#print(dic_yml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_chibe = dic_yml['root_chibe']\n",
    "root_colab = dic_yml['root_colab']\n",
    "root0 = dic_yml['root0']\n",
    "\n",
    "project = dic_yml['project']\n",
    "s_project = dic_yml['s_project']\n",
    "\n",
    "gene_protein = dic_yml['gene_protein']\n",
    "s_omics = dic_yml['s_omics']\n",
    "\n",
    "has_age = dic_yml['has_age']\n",
    "has_gender = dic_yml['has_gender']\n",
    "\n",
    "want_normalized = dic_yml['want_normalized']\n",
    "\n",
    "abs_lfc_cutoff_inf = dic_yml['abs_lfc_cutoff_inf']\n",
    "s_pathw_enrichm_method = dic_yml['s_pathw_enrichm_method']\n",
    "num_min_degs_for_ptw_enr = dic_yml['num_min_degs_for_ptw_enr']\n",
    "tolerance_pathway_index = dic_yml['tolerance_pathway_index']\n",
    "\n",
    "case_list = dic_yml['case_list']\n",
    "case_sel_list = dic_yml['case_sel_list']\n",
    "s_len_case = dic_yml['s_len_case']\n",
    "\n",
    "pval_pathway_cutoff = dic_yml['pval_pathway_cutoff']\n",
    "fdr_pathway_cutoff = dic_yml['fdr_pathway_cutoff']\n",
    "num_of_genes_cutoff = dic_yml['num_of_genes_cutoff']\n",
    "\n",
    "run_list = dic_yml['run_list']\n",
    "chosen_model_list = dic_yml['chosen_model_list']\n",
    "i_dfp_list = dic_yml['i_dfp_list']\n",
    "\n",
    "exp_normalization='quantile_norm' if want_normalized else None\n",
    "normalization='not_normalized' if exp_normalization is None else exp_normalization\n",
    "\n",
    "cfg = Config(project, s_project, case_list, root0)\n",
    "\n",
    "case = case_list[0]\n",
    "\n",
    "n_genes_annot_ptw, n_degs, n_degs_in_ptw, n_degs_not_in_ptw, degs_in_all_ratio = -1,-1,-1,-1,-1\n",
    "abs_lfc_cutoff, fdr_lfc_cutoff, n_degs, n_degs_up, n_degs_dw = cfg.get_best_lfc_cutoff(case, 'not_normalized')\n",
    "\n",
    "\n",
    "print(f\"G/P LFC cutoffs: lfc={abs_lfc_cutoff:.3f}; fdr={fdr_lfc_cutoff:.3f}\")\n",
    "print(f\"Pathway cutoffs: pval={pval_pathway_cutoff:.3f}; fdr={fdr_pathway_cutoff:.3f}; num of genes={num_of_genes_cutoff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpx = Biopax(gene_protein, s_omics, project, s_project, root0,\n",
    "             case_list, has_age, has_gender, clone_objects=False,\n",
    "             exp_normalization=exp_normalization, geneset_num=0, \n",
    "             num_min_degs_for_ptw_enr=num_min_degs_for_ptw_enr, \n",
    "             tolerance_pathway_index=tolerance_pathway_index, \n",
    "             s_pathw_enrichm_method = s_pathw_enrichm_method)\n",
    "\n",
    "case = case_list[0]\n",
    "\n",
    "bpx.cfg.set_default_best_lfc_cutoff(normalization, abs_lfc_cutoff=1, fdr_lfc_cutoff=0.05)\n",
    "ret, degs, degs_ensembl, dfdegs = bpx.open_case(case, verbose=False)\n",
    "print(\"\\nEcho Parameters:\")\n",
    "bpx.echo_parameters()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble: is_seldata=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "is_seldata=False\n",
    "\n",
    "with_gender=bpx.has_gender\n",
    "with_gender_list = [False, True] if with_gender else [False]\n",
    "\n",
    "print(f\"with_gender = {with_gender} because has_gender = {bpx.has_gender}\")\n",
    "###################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiating Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = dic_yml['API_KEY']\n",
    "\n",
    "disease = dic_yml['disease']\n",
    "context_disease = dic_yml['context_disease']\n",
    "n_sentences = dic_yml['n_sentences']\n",
    "chosen_model_sampling = dic_yml['chosen_model_sampling']\n",
    "\n",
    "gem = Gemini(bpx=bpx, is_seldata=is_seldata, disease=disease, context_disease=context_disease, \n",
    "             API_KEY=API_KEY, n_sentences=n_sentences, root0=root0, \n",
    "             chosen_model_list=chosen_model_list, i_dfp_list=i_dfp_list, chosen_model_sampling=chosen_model_sampling)\n",
    "print(\"\\n\")\n",
    "print(gem.disease, gem.is_seldata)\n",
    "print(\"Context:\", context_disease)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_seldata, case_list, s_len_case, run_list, chosen_model_list, i_dfp_list, chosen_model_sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gem.set_case(bpx.case, bpx.df_enr, bpx.df_enr0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gem.is_seldata, gem.bpx.case_list, gem.chosen_model_list, gem.i_dfp_list, gem.chosen_model_sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose=True\n",
    "\n",
    "run='run01'\n",
    "case=case_list[0]\n",
    "print(\">>>\", case, '\\n')\n",
    "\n",
    "dfpiva = gem.open_gemini_dfpiva_all_models_one_run(run=run, chosen_model_list=chosen_model_list, verbose=verbose)\n",
    "dfpiv = dfpiva[(dfpiva.case == case) & (dfpiva.i_dfp == 0)]\n",
    "\n",
    "dfpiv.consensus.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Statistics Default x BCA: confusion table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose=True\n",
    "force=False\n",
    "\n",
    "run='run01'\n",
    "case = case_list[0]\n",
    "# group_discovery_fp_fn_enriched_bca\n",
    "df, conf_list = gem.confusion_table_fp_fn_enriched_bca(run=run, case=case, chosen_model_list=chosen_model_list, force=force, verbose=verbose)\n",
    "\n",
    "print(case, conf_list, '\\n')\n",
    "print(len(df))\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('group').count().reset_index().iloc[:,:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "verbose=False\n",
    "force=False\n",
    "\n",
    "run='run01'\n",
    "\n",
    "for case in case_list:\n",
    "    df, conf_list = gem.confusion_table_fp_fn_enriched_bca(run=run, case=case, chosen_model_list=chosen_model_list, force=force, verbose=verbose)\n",
    "\n",
    "print(case, conf_list, '\\n')\n",
    "print(len(df))\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('group').count().reset_index().iloc[:,:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "force=False\n",
    "verbose=True\n",
    "prompt=False\n",
    "param_perc=0.9\n",
    "\n",
    "# dfa = gem.calc_all_significance_per_run_case_enriched_bca(run=run, case_list=case_list, chosen_model_list=chosen_model_list, \n",
    "dfa = gem.calc_confusion_stats_enriched_bca_per_run_case(run=run, case_list=case_list, chosen_model_list=chosen_model_list, \n",
    "                                                         param_perc=param_perc, prompt=prompt, force=force, verbose=verbose)\n",
    "dfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Statistics G0 x G1: confusion table\n",
    "  - Positive control: i_dfp==0\n",
    "  - fuzzy negative1: i_dfp==1\n",
    "  - fuzzy negative2: i_dfp==2\n",
    "  - negative control: i_dfp==3\n",
    "  - calc:\n",
    "    - TP, FP: positive control\n",
    "    - TN1, FN1: fuzzy negative1\n",
    "    - TN2, FN2: fuzzy negative2\n",
    "    - TN, FN: negative control\n",
    "    - calc: Sensitivity, Specificity, Accuracy, and F1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose=True\n",
    "force=False\n",
    "\n",
    "dfconf1, dfconf2, dfconf3 = \\\n",
    "    gem.calc_gemini_4groups_confusion_table(run_list=run_list, case_list=case_list,\n",
    "                                            chosen_model_list=chosen_model_list,\n",
    "                                            force=force, verbose=verbose)\n",
    "\n",
    "if dfconf1 is None:\n",
    "    print(\"Could not calcualte\")\n",
    "    df3 = pd.DataFrame()\n",
    "else:\n",
    "    cols = ['case', 'which', 'n', 'npos', 'nneg', 'TP', 'FP', 'TN', 'FN', 'sens', 'spec', 'accu', 'prec', 'f1_score']\n",
    "    \n",
    "    run = run_list[0]\n",
    "    \n",
    "    df1 = dfconf1[dfconf1.run == run][cols]\n",
    "    df2 = dfconf2[dfconf2.run == run][cols]\n",
    "    df3 = dfconf3[dfconf3.run == run][cols]\n",
    "    \n",
    "    pd.options.display.float_format = '{:,.3f}'.format\n",
    "\n",
    "    print(\"Ok\")\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:,.3f}'.format\n",
    "\n",
    "print(\">>>\", run, '\\n')\n",
    "mu_f1 = df1.f1_score.mean()\n",
    "std_f1 = df1.f1_score.std()\n",
    "print(f\"G0 x G1: mu_f1 {100*mu_f1:.1f}% ({100*std_f1:.1f}%)\\n\")\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\">>>\", run, '\\n')\n",
    "mu_f1 = df2.f1_score.mean()\n",
    "std_f1 = df2.f1_score.std()\n",
    "print(f\"G0 x G2: mu_f1 {100*mu_f1:.1f}% ({100*std_f1:.1f}%)\\n\")\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\">>>\", run, '\\n')\n",
    "mu_f3 = df3.f1_score.mean()\n",
    "std_f3 = df3.f1_score.std()\n",
    "print(f\"G0 x G3: mu_f3 {100*mu_f3:.1f}% ({100*std_f3:.1f}%)\\n\")\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = [('sens', 'Sensitivity'), ('spec', 'Specificity'), ('accu', 'Accuracy'),\n",
    "\t   ('prec', 'Precision'), ('f1_score', 'F1-score')]\n",
    "\n",
    "for i in range(3):\n",
    "    if i==0:\n",
    "        dfa = df1\n",
    "        dfb = df2\n",
    "        compare = 'G0xG1 x G0xG2'\n",
    "    elif i==1:\n",
    "        dfa = df1\n",
    "        dfb = df3\n",
    "        compare = 'G0xG1 x G0xG3'\n",
    "    else:\n",
    "        dfa = df2\n",
    "        dfb = df3\n",
    "        compare = 'G0xG2 x G0xG3'\n",
    "\n",
    "    for col, test in mat:\n",
    "        # print(dfa[col])\n",
    "        # print(dfb[col])\n",
    "        \n",
    "        mua  = dfa[col].mean()\n",
    "        stda = dfa[col].std()\n",
    "\n",
    "        mub  = dfb[col].mean()\n",
    "        stdb = dfb[col].std()\n",
    "        \n",
    "        s_stat, stat, pval = calc_ttest(dfa[col], dfb[col])\n",
    "\n",
    "        if stat is None:\n",
    "            print(f\"{compare}: {s_stat}\")\n",
    "        else:\n",
    "            print(f\"{compare}: {100*mua:.1f}% ({100*stda:.1f}%) x {100*mub:.1f}% ({100*stdb:.1f}%) -> {test:12} pval {pval:.2e}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_stat, stat, pval = calc_ttest(df1[col], df3[col])\n",
    "s_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_stat, stat, pval = calc_ttest(df2[col], df3[col])\n",
    "s_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['run', 'which', 'case', 'n', 'npos', 'nneg', 'TP', 'FP', 'TN', 'FN', \n",
    "        'sens', 'spec', 'accu', 'prec', 'f1_score']\n",
    "\n",
    "cols = ['run', 'which', 'case', 'n', 'npos', 'nneg', 'TP', 'FP', 'TN', 'FN', \n",
    "        'sens', 'spec', 'accu', 'prec', 'f1_score']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run=run_list[1]\n",
    "\n",
    "df11 = dfconf1[dfconf1.run == run][cols]\n",
    "df12 = dfconf2[dfconf2.run == run][cols]\n",
    "df13 = dfconf3[dfconf3.run == run][cols]\n",
    "\n",
    "print(\">>>\", run, '\\n')\n",
    "df13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run='run01'\n",
    "verbose=True\n",
    "force=True\n",
    "\n",
    "df, df1, df2, df3 = gem.calc_gemini_4groups_confusion_stats(run=run, run_list=run_list,\n",
    "                                                            case_list=case_list, chosen_model_list=chosen_model_list,\n",
    "                                                            alpha=0.05, force=force, verbose=verbose)\n",
    "\n",
    "print(len(df))\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = 'sens'\n",
    "compare = 'G1xG0'\n",
    "df[(df.run == run) & (df.test == col)   ] #  & (df['compare'] == compare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_bar_errors=True\n",
    "\n",
    "fig = gem.barplot_comparing_confusion_groups(run=run, run_list=run_list, case_list=case_list,\n",
    "                                       chosen_model_list=chosen_model_list,\n",
    "                                       width=1100, height=500, \n",
    "                                       fontsize=14, fontcolor='black',\n",
    "                                       margin=dict( l=20, r=20, b=100, t=120, pad=4),\n",
    "                                       plot_bgcolor=\"whitesmoke\",\n",
    "                                       xaxis_title=\"parameters\", yaxis_title='percentage (%)',\n",
    "                                       minus_test=-.2, minus_group=-0.1, \n",
    "                                       annot_fontfamily=\"Arial, monospace\", annot_fontsize=12, \n",
    "                                       annot_fontcolor='black', show_bar_errors=show_bar_errors,\n",
    "                                       savePlot=True, force=force, verbose=verbose)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df))\n",
    "\n",
    "# cols = ['run', 'test', 'test_desc', 'compare', 'n', 'mu', 'std', 'error', 'SEM', 'cinf', 'csup', 'pval', 'asteristics', 'stat', 's_stat']\n",
    "cols = ['test_desc', 'compare', 'n', 'mu', 'std', 'error', 'SEM', 'cinf', 'csup', 'pval', 'pval_bonf', 'asteristics', 'stat', 's_stat']\n",
    "df[df.test_desc == 'Sensitivity'][cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing runs: stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose=True\n",
    "force=False\n",
    "\n",
    "dfstat = gem.calc_stats_gemini_4groups_confusion_compare_runs(run_list=run_list, case_list=case_list,\n",
    "                                                               chosen_model_list=chosen_model_list,\n",
    "                                                               force=force, verbose=verbose)\n",
    "\n",
    "# cols = ['run0', 'run1', 'which', 'test', 'test_ext', 'mu_param0', 'std_param0', 'mu_param1', 'std_param1', 'stat_test', 'pvalue', 'stat', 'text_stat', 'text_ext']\n",
    "dfstat.test_ext.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['which', 'test_ext', 'mu_param0', 'std_param0', 'mu_param1', 'std_param1', 'stat_test', 'pvalue', 'stat', 'text_ext']\n",
    "tests = ['Sensitivity', 'Specificity', 'Accuracy', 'Precision', 'F1-score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfstat[dfstat.test_ext == tests[0]][cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfstat[dfstat.test_ext == tests[1]][cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfstat[dfstat.test_ext == tests[2]][cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfstat[dfstat.test_ext == tests[3]][cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfstat[dfstat.test_ext == tests[4]][cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run='run01'\n",
    "case='WNT'\n",
    "df, conf_list = gem.confusion_table_fp_fn_enriched_bca(run=run, case=case, chosen_model_list=chosen_model_list, force=force, verbose=verbose)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp, fp, tn, fn = conf_list\n",
    "tp, fp, tn, fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfpiva = gem.open_gemini_dfpiva_all_models_one_run(run=run, chosen_model_list=chosen_model_list, \n",
    "                                                    verbose=verbose)\n",
    "dfpiva.i_dfp.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Development & tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "force=False\n",
    "verbose=False\n",
    "param_perc=0.9\n",
    "\n",
    "prompt=False\n",
    "\n",
    "run='run01'\n",
    "print(f\"run {run} (chi-square statistics) param_perc={param_perc}:\")\n",
    "\n",
    "dic={}; icount=-1\n",
    "for case in case_list:\n",
    "    df, s_stat, stat, pvalue, dof, expected, list_obs, list_exp, Npos, Nneg = \\\n",
    "    gem.calc_significance_per_run_case_enriched_bca(run=run, case=case, chosen_model_list=chosen_model_list, \n",
    "                                                    param_perc=param_perc, force=force, verbose=verbose)\n",
    "\n",
    "    if prompt:\n",
    "        expected = list([ list(np.round(x,1)) for x in expected])\n",
    "        print(f\"\\t{case:15} stat {str(np.round(stat,2)):7} pval {pvalue:.2e} - {str(list_obs):8} x {str(list_exp):8}; exp: {expected}\")\n",
    "\n",
    "    icount += 1\n",
    "    dic[icount] = {}\n",
    "    dic2 = dic[icount]\n",
    "\n",
    "    dic2['run'] = run\n",
    "    dic2['case']  = case\n",
    "\n",
    "    dic2['TP'] = list_obs[0]\n",
    "    dic2['FP'] = list_obs[1]\n",
    "    dic2['TN'] = list_obs[2]\n",
    "    dic2['FN'] = list_obs[3]\n",
    "\n",
    "    dic2['TP_exp'] = list_exp[0]\n",
    "    dic2['FP_exp'] = list_exp[1]\n",
    "    dic2['TN_exp'] = list_exp[2]\n",
    "    dic2['FN_exp'] = list_exp[3]\n",
    "\n",
    "    dic2['stat'] = stat\n",
    "    dic2['pvalue'] = pvalue\n",
    "    dic2['dof'] = dof\n",
    "    dic2['expected'] = np.round(expected, 2)\n",
    "\n",
    "dfa = pd.DataFrame(dic).T\n",
    "dfa['fdr'] = fdr(dfa.pvalue)\n",
    "\n",
    "# cols = ['run', 'case', 'TP', 'FP', 'TN', 'FN', 'TP_exp', 'FP_exp', 'TN_exp', 'FN_exp', 'stat', 'pvalue', 'dof', 'expected', 'fdr']\n",
    "cols = ['run', 'case', 'fdr', 'pvalue', 'TP', 'FP', 'TN', 'FN', 'TP_exp', 'FP_exp', 'TN_exp', 'FN_exp', 'expected']\n",
    "dfa[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def barplot_comparing_groups(run:str, df1:pd.DataFrame, df2:pd.DataFrame, df3:pd.DataFrame, \n",
    "                             width:int=1100, height:int=600, \n",
    "                             fontsize:int=14, fontcolor:str='black',\n",
    "                             margin:dict=dict( l=20, r=20, b=100, t=120, pad=4), plot_bgcolor:str=\"whitesmoke\",\n",
    "                             xaxis_title:str=\"parameters\", yaxis_title:str='percentage (%)',\n",
    "                             minus_test:float=-.1, minus_group:float=-0.05, \n",
    "                             annot_fontfamily:str=\"Arial, monospace\", annot_fontsize:int=12, \n",
    "                             annot_fontcolor:str='black', savePlot:bool=True, alpha=0.05, verbose:bool=False):\n",
    "    \n",
    "\n",
    "    cols = ['case', 'which', 'n', 'npos', 'nneg', 'TP', 'FP', 'TN', 'FN', 'sens', 'spec', 'accu', 'prec', 'f1_score']\n",
    "    \n",
    "    df1 = dfconf1[dfconf1.run == run][cols]\n",
    "    df2 = dfconf2[dfconf2.run == run][cols]\n",
    "    df3 = dfconf3[dfconf3.run == run][cols]\n",
    "\n",
    "\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    title=f\"Statistical parameters for Gi x G0 run {run}\"\n",
    "    x_num = 0\n",
    "\n",
    "    mat = [('sens', 'Sensitivity'), ('spec', 'Specificity'), ('accu', 'Accuracy'),\n",
    "    \t   ('prec', 'Precision'), ('f1_score', 'F1-score')]\n",
    "    # \"aliceblue\", \"azure\",\n",
    "    colors_blue = [\"blue\", \"blueviolet\", \"navy\", \"darkblue\", \"darkcyan\", ]\n",
    "    \n",
    "    x_num=0\n",
    "    dic={}; icount=-1\n",
    "    for col, test in mat:\n",
    "\n",
    "        vals_G0G3 = df3[col]\n",
    "        \n",
    "        for i in range(3):\n",
    "            if i==0:\n",
    "                dfa = df1\n",
    "                compare = 'G1xG0'\n",
    "            elif i==1:\n",
    "                dfa = df2\n",
    "                compare = 'G2xG0'\n",
    "            else:\n",
    "                dfa = df3\n",
    "                compare = 'G3xG0'\n",
    "\n",
    "            na = len(dfa)\n",
    "            mua  = dfa[col].mean()\n",
    "            stda = dfa[col].std()\n",
    "\n",
    "            # alpha/2 Bonferroni correction, 2 comparisons.\n",
    "            error, cinf, csup, SEM, stri =calc_confidence_interval_param(mua, stda, na, alpha=alpha/2, two_tailed=True)\n",
    "            \n",
    "            icount += 1\n",
    "            dic[icount] = {}\n",
    "            dic2 = dic[icount]\n",
    "\n",
    "            dic2['run'] = run\n",
    "            dic2['test'] = col\n",
    "            dic2['test_desc'] = test\n",
    "            dic2['compare'] = compare\n",
    "            dic2['n'] = na\n",
    "            dic2['mu'] = mua\n",
    "            dic2['std'] = stda\n",
    "            dic2['error'] = error\n",
    "            dic2['SEM'] = SEM\n",
    "            dic2['cinf'] = cinf\n",
    "            dic2['csup'] = csup\n",
    "            \n",
    "            x_num += 1\n",
    "            label = f\"{compare}-{col}\"\n",
    "            fig.add_trace(go.Bar(x=[x_num], y=[mua], marker_color=colors_blue[i],\n",
    "                                 error_y=dict(type='data', array=[error]), name=label))\n",
    "\n",
    "            fig.add_annotation(\n",
    "                x=x_num,\n",
    "                y=minus_group,\n",
    "                xref=\"x\",\n",
    "                yref=\"y\",\n",
    "                text=compare,\n",
    "                showarrow=False,\n",
    "                font=dict(\n",
    "                    family=\"Arial, monospace\",\n",
    "                    size=fontsize,\n",
    "                    color=fontcolor\n",
    "                    ),\n",
    "                align=\"center\", )\n",
    "            \n",
    "            if i == 2:\n",
    "                fig.add_annotation(\n",
    "                    x=x_num-1,\n",
    "                    y=minus_test,\n",
    "                    xref=\"x\",\n",
    "                    yref=\"y\",\n",
    "                    text=test,\n",
    "                    showarrow=False,\n",
    "                    font=dict(\n",
    "                        family=\"Arial, monospace\",\n",
    "                        size=fontsize,\n",
    "                        color=fontcolor\n",
    "                        ),\n",
    "                    align=\"center\", )\n",
    "\n",
    "                x_num+=1\n",
    "\n",
    "                dic2['pval'] = None\n",
    "                dic2['asteristics'] = None\n",
    "                dic2['stat'] = None\n",
    "                dic2['s_stat'] = None\n",
    "\n",
    "            else:\n",
    "                s_stat, stat, pval = calc_ttest(dfa[col], vals_G0G3)\n",
    "                aster = stat_asteristics(pval,NS='--')\n",
    "\n",
    "                dic2['pval'] = pval\n",
    "                dic2['asteristics'] = aster\n",
    "                dic2['stat'] = stat\n",
    "                dic2['s_stat'] = s_stat\n",
    "                \n",
    "                fig.add_annotation(\n",
    "                    x=x_num,\n",
    "                    y=mua+error+0.05,\n",
    "                    xref=\"x\",\n",
    "                    yref=\"y\",\n",
    "                    text=f\"{aster}\", # <br>{pval:.2e}\n",
    "                    showarrow=False,\n",
    "                    font=dict(\n",
    "                        family=\"Arial, monospace\",\n",
    "                        size=fontsize,\n",
    "                        color=fontcolor\n",
    "                        ),\n",
    "                    align=\"center\", )\n",
    "                    \n",
    "    fig.update_layout(\n",
    "                autosize=True,\n",
    "                title=title,\n",
    "                width=width,\n",
    "                height=height,\n",
    "                plot_bgcolor=plot_bgcolor,\n",
    "                xaxis_title=xaxis_title,\n",
    "                yaxis_title=yaxis_title,\n",
    "                xaxis_showticklabels=False,\n",
    "                # yaxis_range=[0, 1],\n",
    "                showlegend=False,\n",
    "                font=dict(\n",
    "                    family=\"Arial\",\n",
    "                    size=14,\n",
    "                    color=\"Black\"\n",
    "                ),\n",
    "    )\n",
    "\n",
    "    df = pd.DataFrame(dic).T\n",
    "\n",
    "    return fig, df\n",
    "\n",
    "run='run01'\n",
    "verbose=False\n",
    "\n",
    "dfconf1, dfconf2, dfconf3 = \\\n",
    "    gem.calc_gemini_4groups_confusion_table(run_list=run_list, case_list=case_list,\n",
    "                                            chosen_model_list=chosen_model_list,\n",
    "                                            force=force, verbose=verbose)\n",
    "\n",
    "fig, df = barplot_comparing_groups(run=run, df1=dfconf1, df2=dfconf2, df3=dfconf3, \n",
    "                                   width=1100, height=600, \n",
    "                                   fontsize=14, fontcolor='black',\n",
    "                                   margin=dict( l=20, r=20, b=100, t=120, pad=4),\n",
    "                                   plot_bgcolor=\"whitesmoke\",\n",
    "                                   xaxis_title=\"parameters\", yaxis_title='percentage (%)',\n",
    "                                   minus_test=-.1, minus_group=-0.05, \n",
    "                                   annot_fontfamily=\"Arial, monospace\", annot_fontsize=12, \n",
    "                                   annot_fontcolor='black', savePlot=True, verbose=False)\n",
    "\n",
    "fig.show()\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
